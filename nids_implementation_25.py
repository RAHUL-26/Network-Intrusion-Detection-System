# -*- coding: utf-8 -*-
"""NIDS_implementation_25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nJS7VEWyJ-ALI55Nwzk5nPcaTJ553uhv
"""

! pip install -q kaggle

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d cicdataset/cicids2017

! unzip /content/cicids2017.zip

import numpy as np
import os
import pandas as pd
import time
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns
from imblearn.under_sampling import RandomUnderSampler
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn import tree
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn import metrics
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
low_memory=False
df1=pd.read_csv("/content/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv")
df2=pd.read_csv("/content/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv")
df3=pd.read_csv("/content/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv")
df4=pd.read_csv("/content/MachineLearningCSV/MachineLearningCVE/Monday-WorkingHours.pcap_ISCX.csv")
df5=pd.read_csv("/content/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv")
df6=pd.read_csv("/content/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv")
df7=pd.read_csv("/content/MachineLearningCSV/MachineLearningCVE/Tuesday-WorkingHours.pcap_ISCX.csv")
df8=pd.read_csv("/content/MachineLearningCSV/MachineLearningCVE/Wednesday-workingHours.pcap_ISCX.csv")

nRowsRead = None
df1_sampled = df1.sample(frac=0.25, random_state=0)
df2_sampled = df2.sample(frac=0.25, random_state=0)
df3_sampled = df3.sample(frac=0.25, random_state=0)
df4_sampled = df4.sample(frac=0.25, random_state=0)

# Concatenate the sampled datasets
df = pd.concat([df1_sampled, df2_sampled, df3_sampled, df4_sampled])

df.shape

df[' Label'].value_counts()

df.duplicated().sum()

df.shape

df =  df.drop_duplicates(keep="first")

df.duplicated().sum()

df.shape

df.isnull().sum().sort_values(ascending = False)

df.dropna(inplace=True)

df.shape

df.isnull().sum().sort_values(ascending = False)

df=df.groupby(' Label').filter(lambda x:len(x)>10000)
df[' Label'].value_counts()

integer = []
f = []
for i in df.columns[:-1]:
    if df[i].dtype == "int64": integer.append(i)
    else : f.append(i)

df[integer] = df[integer].astype("int32")
df[f] = df[f].astype("float32")

df.shape

df = df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]
# df.reset_index(drop=True,inplace=True)

df.shape

def correlation(dataset, threshold):
    col_corr = set()
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
              colname = corr_matrix.columns[i]
              col_corr.add(colname)
    return col_corr

corr_features = correlation(df, 0.85)
corr_features

df.drop(corr_features,axis=1,inplace=True)

x = df.drop([' Label'],axis=1)
y = df[' Label']

x.head()

y.head()

rus = RandomUnderSampler(random_state=0)
rus.fit(x, y)
Xn, yn = rus.fit_resample(x, y)

Xn.head()
Xn.shape

cols = list(Xn.columns)
for col in cols:
    Xn[col] = stats.zscore(Xn[col])

from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
X_train, X_test, Y_train, Y_test = train_test_split(Xn,yn,test_size=0.30,random_state=0)

from sklearn.impute import SimpleImputer

# Create an instance of SimpleImputer with 'mean' strategy to replace NaN values
imputer = SimpleImputer(strategy='mean')

# Fit the imputer to X_train and transform X_train and X_test with it
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Apply StandardScaler to X_train and X_test
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.impute import SimpleImputer

# Create an instance of SimpleImputer with 'mean' strategy to replace NaN values
imputer = SimpleImputer(strategy='mean')

# Fit the imputer to X_train and transform X_train and X_test with it
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# Apply StandardScaler to X_train and X_test
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.ensemble import AdaBoostClassifier as ada
start = time.time()
ada_boost_model = ada(n_estimators=50,
                         learning_rate=1)
ada_boost_model.fit(X_train, Y_train)
print("Time taken to train model: ", time.time()-start," seconds")

from sklearn import metrics
Predict_X =  ada_boost_model.predict(X_train)
scores = cross_val_score(ada_boost_model, X_train, Y_train, cv=7)
accuracy = metrics.accuracy_score(Y_train,Predict_X)
confusion_matrix = metrics.confusion_matrix(Y_train, Predict_X)
classification = metrics.classification_report(Y_train, Predict_X)

print()
print('--------------------------- Results --------------------------------')
print()
print ("Cross Validation Mean Score:" "\n", scores.mean())
print()
print ("Model Accuracy:" "\n", accuracy)
print()
print("Confusion matrix:" "\n", confusion_matrix)
print()
print("Classification report:" "\n", classification)
print()

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier as RFC
start = time.time()
Random_Forest = RFC(max_depth=40)
Random_Forest.fit(X_train, Y_train)
print("Time taken to train model: ", time.time()-start," seconds")

from sklearn import metrics
Predict_X =  Random_Forest.predict(X_train)
scores = cross_val_score(Random_Forest, X_train, Y_train, cv=7)
accuracy_rf = metrics.accuracy_score(Y_train,Predict_X)
confusion_matrix_rf = metrics.confusion_matrix(Y_train, Predict_X)
classification_rf = metrics.classification_report(Y_train, Predict_X)

print()
print('--------------------------- Results --------------------------------')
print()
print ("Cross Validation Mean Score:" "\n", scores.mean())
print()
print ("Model Accuracy:" "\n", accuracy_rf)
print()
print("Confusion matrix:" "\n", confusion_matrix_rf)
print()
print("Classification report:" "\n", classification_rf)
print()

def plot_confusion_matrix(cm,title,cmap=None,target=None,normalize=False):

    import itertools
    accuracy = np.trace(cm) / float(np.sum(cm))
    misclass = 1 - accuracy

    if cmap is None:
        cmap = plt.get_cmap('viridis')
    plt.figure(figsize=(20, 20))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()

    if target is not None:
        ticks = np.arange(len(target))
        plt.xticks(ticks, target, rotation=45)
        plt.yticks(ticks, target)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]


    thresh = cm.max() / 1.5 if normalize else cm.max() / 2
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        if normalize:
            plt.text(j, i, "{:0.4f}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="black" if cm[i, j] > thresh else "white")
        else:
            plt.text(j, i, "{:,}".format(cm[i, j]),
                     horizontalalignment="center",
                     color="black" if cm[i, j] > thresh else "white")
    plt.grid(False)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))
    plt.show()
    plt.savefig(title, bbox_inches='tight', dpi=300)

plot_confusion_matrix(cm=confusion_matrix_rf ,title= 'Random Forest Classification')

Predict_X =  Random_Forest.predict(X_test)
scores = cross_val_score(Random_Forest, X_test, Y_test, cv=7)
accuracy_rf = metrics.accuracy_score(Y_test,Predict_X)
confusion_matrix_rf = metrics.confusion_matrix(Y_test, Predict_X)
classification_rf = metrics.classification_report(Y_test, Predict_X)

print()
print('--------------------------- Results --------------------------------')
print()
print ("Cross Validation Mean Score:" "\n", scores.mean())
print()
print ("Model Accuracy:" "\n", accuracy_rf)
print()
print("Confusion matrix:" "\n", confusion_matrix_rf)
print()
print("Classification report:" "\n", classification_rf)
print()

plot_confusion_matrix(cm=confusion_matrix_rf ,title= 'Random Forest Classification')

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

# Create an instance of the KNN model
knn_model = KNeighborsClassifier()

# Train the KNN model
knn_model.fit(X_train, Y_train)

# Predict the labels for the test data
predict_X_knn = knn_model.predict(X_test)

# Calculate the cross-validation scores
scores_knn = cross_val_score(knn_model, X_test, Y_test, cv=7)

# Compute the accuracy score
accuracy_knn = metrics.accuracy_score(Y_test, predict_X_knn)

# Construct the confusion matrix
confusion_matrix_knn = metrics.confusion_matrix(Y_test, predict_X_knn)

# Generate the classification report
classification_knn = metrics.classification_report(Y_test, predict_X_knn)

# Print the results
print()
print('--------------------------- Results --------------------------------')
print()
print("Cross Validation Mean Score:\n", scores_knn.mean())
print()
print("Model Accuracy:\n", accuracy_knn)
print()
print("Confusion matrix:\n", confusion_matrix_knn)
print()
print("Classification report:\n", classification_knn)
print()

plot_confusion_matrix(cm=confusion_matrix_knn ,title= 'Random Forest Classification')

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

# Create an instance of the Decision Tree model
dt_model = DecisionTreeClassifier()

# Train the Decision Tree model
dt_model.fit(X_train, Y_train)

# Predict the labels for the test data
predict_X_dt = dt_model.predict(X_test)

# Calculate the cross-validation scores
scores_dt = cross_val_score(dt_model, X_test, Y_test, cv=7)

# Compute the accuracy score
accuracy_dt = metrics.accuracy_score(Y_test, predict_X_dt)

# Construct the confusion matrix
confusion_matrix_dt = metrics.confusion_matrix(Y_test, predict_X_dt)

# Generate the classification report
classification_dt = metrics.classification_report(Y_test, predict_X_dt)

# Print the results
print()
print('--------------------------- Results --------------------------------')
print()
print("Cross Validation Mean Score:\n", scores_dt.mean())
print()
print("Model Accuracy:\n", accuracy_dt)
print()
print("Confusion matrix:\n", confusion_matrix_dt)
print()
print("Classification report:\n", classification_dt)
print()

plot_confusion_matrix(cm=confusion_matrix_dt ,title= 'Random Forest Classification')

from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

# Create an instance of the SVM model
svm_model = SVC()

# Train the SVM model
svm_model.fit(X_train, Y_train)

# Predict the labels for the test data
predict_X_svm = svm_model.predict(X_test)

# Calculate the cross-validation scores
scores_svm = cross_val_score(svm_model, X_test, Y_test, cv=7)

# Compute the accuracy score
accuracy_svm = metrics.accuracy_score(Y_test, predict_X_svm)

# Construct the confusion matrix
confusion_matrix_svm = metrics.confusion_matrix(Y_test, predict_X_svm)

# Generate the classification report
classification_svm = metrics.classification_report(Y_test, predict_X_svm)

# Print the results
print()
print('--------------------------- Results --------------------------------')
print()
print("Cross Validation Mean Score:\n", scores_svm.mean())
print()
print("Model Accuracy:\n", accuracy_svm)
print()
print("Confusion matrix:\n", confusion_matrix_svm)
print()
print("Classification report:\n", classification_svm)
print()

plot_confusion_matrix(cm=confusion_matrix_svm ,title= 'Random Forest Classification')

# Generate Features vs Accuracy graph
num_features = np.arange(1, X_train.shape[1] + 1)
accuracy_scores = []

for num in num_features:
    selected_features = X_train[:, :num]
    knn_model.fit(selected_features, Y_train)
    predict_X_knn = knn_model.predict(X_test[:, :num])
    accuracy = metrics.accuracy_score(Y_test, predict_X_knn)
    accuracy_scores.append(accuracy)

plt.plot(num_features, accuracy_scores, marker='o')
plt.xlabel('Number of Features')
plt.ylabel('Accuracy')
plt.title('Features vs Accuracy (KNN)')
plt.show()

# Generate Accuracy vs Classifier graph
classifiers = ['KNN', 'Random Forest', 'SVM']
accuracy_scores = [accuracy_knn, accuracy_rf, accuracy_svm]

plt.bar(classifiers, accuracy_scores)
plt.xlabel('Classifier')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Classifier')
plt.show()

# Generate Accuracy vs False Positive/Negative graph
false_positive_rate = confusion_matrix_knn[0, 1] / np.sum(confusion_matrix_knn[0])
false_negative_rate = confusion_matrix_knn[1, 0] / np.sum(confusion_matrix_knn[1])

categories = ['False Positive', 'False Negative']
rates = [false_positive_rate, false_negative_rate]

plt.bar(categories, rates)
plt.xlabel('Error Type')
plt.ylabel('Rate')
plt.title('Accuracy vs False Positive/Negative')
plt.show()

# Generate Features vs Accuracy graph
num_features = np.arange(1, X_train.shape[1] + 1)
accuracy_scores = []

for num in num_features:
    selected_features = X_train[:, :num]
    svm_model.fit(selected_features, Y_train)
    predict_X_svm = svm_model.predict(X_test[:, :num])
    accuracy = metrics.accuracy_score(Y_test, predict_X_svm)
    accuracy_scores.append(accuracy)

plt.plot(num_features, accuracy_scores, marker='o')
plt.xlabel('Number of Features')
plt.ylabel('Accuracy')
plt.title('Features vs Accuracy (SVM)')
plt.show()

# Generate Accuracy vs Classifier graph
classifiers = ['KNN', 'Random Forest', 'SVM']
accuracy_scores = [accuracy_knn, accuracy_rf, accuracy_svm]

plt.bar(classifiers, accuracy_scores)
plt.xlabel('Classifier')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Classifier')
plt.show()

# Generate Accuracy vs False Positive/Negative graph
false_positive_rate = confusion_matrix_svm[0, 1] / np.sum(confusion_matrix_svm[0])
false_negative_rate = confusion_matrix_svm[1, 0] / np.sum(confusion_matrix_svm[1])

categories = ['False Positive', 'False Negative']
rates = [false_positive_rate, false_negative_rate]

plt.bar(categories, rates)
plt.xlabel('Error Type')
plt.ylabel('Rate')
plt.title('Accuracy vs False Positive/Negative')
plt.show()

# Generate Features vs Accuracy graph
num_features = np.arange(1, X_train.shape[1] + 1)
accuracy_scores = []

for num in num_features:
    selected_features = X_train[:, :num]
    Random_Forest.fit(selected_features, Y_train)
    predict_X_rf = Random_Forest.predict(X_test[:, :num])
    accuracy = metrics.accuracy_score(Y_test, predict_X_rf)
    accuracy_scores.append(accuracy)

plt.plot(num_features, accuracy_scores, marker='o')
plt.xlabel('Number of Features')
plt.ylabel('Accuracy')
plt.title('Features vs Accuracy (Random Forest)')
plt.show()

# Generate Accuracy vs Classifier graph
classifiers = ['KNN', 'Random Forest', 'SVM']
accuracy_scores = [accuracy_knn, accuracy_rf, accuracy_svm]

plt.bar(classifiers, accuracy_scores)
plt.xlabel('Classifier')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Classifier')
plt.show()

# Generate Accuracy vs False Positive/Negative graph
false_positive_rate = confusion_matrix_rf[0, 1] / np.sum(confusion_matrix_rf[0])
false_negative_rate = confusion_matrix_rf[1, 0] / np.sum(confusion_matrix_rf[1])

categories = ['False Positive', 'False Negative']
rates = [false_positive_rate, false_negative_rate]

plt.bar(categories, rates)
plt.xlabel('Error Type')
plt.ylabel('Rate')
plt.title('Accuracy vs False Positive/Negative')
plt.show()

# Generate Features vs Accuracy graph
num_features = np.arange(1, X_train.shape[1] + 1)
accuracy_scores = []

for num in num_features:
    selected_features = X_train[:, :num]
    dt_model.fit(selected_features, Y_train)
    predict_X_dt = dt_model.predict(X_test[:, :num])
    accuracy = metrics.accuracy_score(Y_test, predict_X_dt)
    accuracy_scores.append(accuracy)

plt.plot(num_features, accuracy_scores, marker='o')
plt.xlabel('Number of Features')
plt.ylabel('Accuracy')
plt.title('Features vs Accuracy (Decision Tree)')
plt.show()

# Generate Accuracy vs Classifier graph
classifiers = ['KNN', 'Random Forest', 'SVM', 'Decision Tree']
accuracy_scores = [accuracy_knn, accuracy_rf, accuracy_svm, accuracy_dt]

plt.bar(classifiers, accuracy_scores)
plt.xlabel('Classifier')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Classifier')
plt.show()

# Generate Accuracy vs False Positive/Negative graph
false_positive_rate = confusion_matrix_dt[0, 1] / np.sum(confusion_matrix_dt[0])
false_negative_rate = confusion_matrix_dt[1, 0] / np.sum(confusion_matrix_dt[1])

categories = ['False Positive', 'False Negative']
rates = [false_positive_rate, false_negative_rate]

plt.bar(categories, rates)
plt.xlabel('Error Type')
plt.ylabel('Rate')
plt.title('Accuracy vs False Positive/Negative')
plt.show()

